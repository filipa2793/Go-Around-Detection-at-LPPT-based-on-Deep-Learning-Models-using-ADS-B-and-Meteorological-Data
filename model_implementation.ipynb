{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a87bdaf",
   "metadata": {},
   "source": [
    "# Gate Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d9ba25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the final dataset with all the months and assigning a flight index to each flight within the final dataset (landing index is assigned to each flight within each month)\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "file_paths = sorted(glob.glob(\"*.csv\")) \n",
    "all_dfs = []\n",
    "next_index = 0\n",
    "\n",
    "for path in file_paths:\n",
    "    df = pd.read_csv(path, dtype={\"ICAO\": str})\n",
    "\n",
    "    unique_landings = sorted(df[\"Landing_Index\"].unique())\n",
    "    mapping = {old: new for new, old in enumerate(unique_landings, start=next_index + 1)}\n",
    "    df[\"Flight_Index\"] = df[\"Landing_Index\"].map(mapping)\n",
    "\n",
    "    next_index += len(unique_landings)\n",
    "\n",
    "    all_dfs.append(df)\n",
    "\n",
    "full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "full_df.to_csv(\"all_months.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gate segmentation (paper): 40 gates (0.25 NM) and interpolation, each gate has features\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "gates_nm = np.linspace(10, 0.25, 40) \n",
    "n_gates = len(gates_nm)\n",
    "\n",
    "static_features = [\"Wind_Speed_M\", \"Wind_Gust_E\", \"Wind_Shear_E\", \"C_1000_E\", \"C_500_E\", \"WS_M\", \"FG_M\", \"RA_M\", \"G_M\", \"Wind_CompU_E\", \"Wind_CompV_E\", \"LOS\", \"GA_before\", \"GA_hourly\", \"GA_lasttime\", \"Speed_Diff_lt\", \"Alt_Diff_lt\", \"Lead_Aircraft\"]\n",
    "\n",
    "dynamic_features = [\"Correct Altitude\", \"Speed\", \"Vertical Rate\", \"Glideslope\", \"Deviation\", \"Energy\"]\n",
    "\n",
    "all_features = static_features + dynamic_features\n",
    "n_features = len(all_features)\n",
    "\n",
    "df = full_df\n",
    "\n",
    "X_per_flight, y_per_flight, mask_per_flight, ids_per_flight = [], [], [], []\n",
    "examples_ga, examples_normal = [], []\n",
    "\n",
    "for flight_index, flight_df in df.groupby(\"Flight_Index\"):\n",
    "    flight_label = flight_df[\"Go_Around_Label\"].iloc[0]  \n",
    "    flight_df = flight_df.sort_values(\"Distance_NM\", ascending=True) \n",
    "    flight_df = flight_df.drop_duplicates(subset=\"Distance_NM\", keep=\"first\") \n",
    "\n",
    "    if flight_df.empty or flight_df[\"Distance_NM\"].nunique() < 2: \n",
    "            continue\n",
    "\n",
    "    #dynamic features (time dependent): interpolation for the 40 data points\n",
    "    dynamic_sequence = []\n",
    "    for feat in dynamic_features:\n",
    "        try:\n",
    "            f1 = interp1d(flight_df[\"Distance_NM\"], flight_df[feat], kind=\"linear\", bounds_error=False, fill_value=(flight_df[feat].iloc[0], flight_df[feat].iloc[-1]))\n",
    "            y1 = f1(np.sort(gates_nm))   \n",
    "            y1 = y1[::-1]  \n",
    "        except Exception:\n",
    "            y1 = np.full(n_gates, np.nan)\n",
    "        dynamic_sequence.append(y1)\n",
    "    dynamic_sequence = np.stack(dynamic_sequence , axis=1) #shape: (40, # dynamic features)\n",
    "\n",
    "    #static features (time independent): all the gates have the same value\n",
    "    static_vals = flight_df[static_features].iloc[0].values\n",
    "    static_sequence = np.tile(static_vals, (n_gates, 1)) #shape: (40, # static features)\n",
    "\n",
    "    full_sequence = np.hstack([static_sequence, dynamic_sequence]) \n",
    "    full_sequence = np.nan_to_num(full_sequence, nan=0.0) #shape: (40, # features)\n",
    "    flight_mask = np.ones(n_gates, dtype=int)\n",
    "\n",
    "    #ga flights\n",
    "    if flight_label == 1:\n",
    "        vertrate = full_sequence[:, all_features.index(\"Vertical Rate\")]\n",
    "\n",
    "        if np.all(vertrate >= 0):\n",
    "            continue\n",
    "\n",
    "        first_descent = np.where(vertrate < 0)[0]\n",
    "        if len(first_descent) == 0:\n",
    "            continue\n",
    "        first_descent = first_descent[0]\n",
    "\n",
    "        climb_after_descent = np.where((np.arange(len(vertrate)) >= first_descent) & (vertrate > 0))[0]\n",
    "        if len(climb_after_descent) > 0:\n",
    "            cut_start = climb_after_descent[0]\n",
    "            full_sequence[cut_start:, :] = 0.0\n",
    "            flight_mask[cut_start:] = 0\n",
    "\n",
    "    #normal flights \n",
    "    if flight_label == 0: \n",
    "        vertrate = full_sequence[:, all_features.index(\"Vertical Rate\")] \n",
    "        \n",
    "        first_descent = np.where(vertrate < 0)[0]\n",
    "        if len(first_descent) > 0:\n",
    "            first_descent = first_descent[0]\n",
    "            climb_after_descent = np.where((np.arange(len(vertrate)) >= first_descent) & (vertrate > 0))[0]\n",
    "            if len(climb_after_descent) > 0:\n",
    "                print(f\"Skipping normal flight {flight_index} due to climb after descent\")\n",
    "                continue\n",
    "    \n",
    "    X_per_flight.append(full_sequence) \n",
    "    y_per_flight.append(flight_label) \n",
    "    mask_per_flight.append(flight_mask) \n",
    "    ids_per_flight.append(flight_index)\n",
    "\n",
    "X = np.array(X_per_flight) #shape: (# flights, 20, # features) -> features\n",
    "y = np.array(y_per_flight) #shape: (# flights,) -> unique per flight\n",
    "masks = np.array(mask_per_flight) #shape: (# flights, 20) -> unique per gate\n",
    "ids = np.array(ids_per_flight)\n",
    "\n",
    "for i, feature in enumerate(all_features):\n",
    "    values = X[:,:,i][masks == 1] \n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    print(f\"{feature}: mean={mean:.2f}, std={std:.2f}, min={min_val:.2f}, max={max_val:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e2119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#train, validation (70/20/10)\n",
    "#stratify ensures equal separation btw normal landings and gas\n",
    "#split 1: 90% to train and val and 10% to test\n",
    "X_temp, X_test_real, y_temp, y_test, m_temp, m_test, ids_temp, ids_test = train_test_split(X, y, masks, ids, stratify=y, test_size=0.1, random_state=42)\n",
    "\n",
    "#split 2: 70% to train and 20% to set\n",
    "X_train, X_val, y_train, y_val, m_train, m_val, ids_train, ids_val = train_test_split(X_temp, y_temp, m_temp, ids_temp, stratify=y_temp, test_size=2/9, random_state=42)\n",
    "\n",
    "for i, feature in enumerate(all_features):\n",
    "    values = X_train[:,:,i][m_train == 1] \n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values)\n",
    "    min_val = np.min(values)\n",
    "    max_val = np.max(values)\n",
    "    print(f\"{feature}: mean={mean:.2f}, std={std:.2f}, min={min_val:.2f}, max={max_val:.2f}\")\n",
    "\n",
    "#data normalization\n",
    "scaler = StandardScaler()\n",
    "#X_train shape: (# flights, 40, # features)\n",
    "#m_train shape: (# flights, 40) - which gates are real and which ones are padding\n",
    "X_train_flat = X_train[m_train == 1].reshape(-1, n_features) #just the gates that are being used (not padded)\n",
    "scaler.fit(X_train_flat) #learns the parameters (mean and std)\n",
    "\n",
    "#scalers work with 2D arrays (# samples, # features)\n",
    "def scale_and_reshape(X, mask):\n",
    "    X_flat = X.reshape(-1, n_features) #shape: (# flights*40, # features)\n",
    "    X_scaled = X_flat.copy()\n",
    "    masked_indices = mask.flatten() == 1 #mask.flatten() shape: (# flights*40)\n",
    "    X_scaled[masked_indices] = scaler.transform(X_flat[masked_indices])\n",
    "    return X_scaled.reshape(X.shape)\n",
    "\n",
    "X_train = scale_and_reshape(X_train, m_train)\n",
    "X_test = scale_and_reshape(X_test_real, m_test)\n",
    "X_val = scale_and_reshape(X_val, m_val)\n",
    "\n",
    "summary = {\n",
    "    \"X_train_shape\": X_train.shape,\n",
    "    \"X_val_shape\": X_val.shape,\n",
    "    \"X_test_shape\": X_test.shape,\n",
    "    \"y_train_shape\": y_train.shape,\n",
    "    \"y_val_shape\": y_val.shape,\n",
    "    \"y_test_shape\": y_test.shape,\n",
    "    \"m_train_shape\": m_train.shape,\n",
    "    \"m_val_shape\": m_val.shape,\n",
    "    \"m_test_shape\": m_test.shape,\n",
    "    \"mask_shape\": masks.shape,\n",
    "}\n",
    "\n",
    "num_examples = 5\n",
    "ga_indices = [i for i, y in enumerate(y_per_flight) if y == 1]\n",
    "\n",
    "for i in ga_indices[:num_examples]:\n",
    "    flight_index = ids_per_flight[i]\n",
    "    flight_sequence = X_per_flight[i]\n",
    "    flight_mask = mask_per_flight[i]\n",
    "    example_df = pd.DataFrame(flight_sequence, columns=all_features, index=[f\"{g:.2f}NM\" for g in gates_nm])\n",
    "    example_df[\"Mask\"] = flight_mask  \n",
    "\n",
    "    print(f\"\\n--- GA Flight {flight_index} ---\")\n",
    "    print(example_df)\n",
    "\n",
    "normal_indices = [i for i, y in enumerate(y_per_flight) if y == 0]\n",
    "\n",
    "for i in normal_indices[:num_examples]:\n",
    "    flight_index = ids_per_flight[i]\n",
    "    flight_sequence = X_per_flight[i]\n",
    "    flight_mask = mask_per_flight[i]\n",
    "    example_df = pd.DataFrame(flight_sequence, columns=all_features, index=[f\"{g:.2f}NM\" for g in gates_nm])\n",
    "    example_df[\"Mask\"] = 1\n",
    "\n",
    "    print(f\"\\n--- NORMAL Flight {flight_index} ---\")\n",
    "    print(example_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d09582",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gas = sum(y_per_flight)             \n",
    "num_normals = len(y_per_flight) - num_gas\n",
    "\n",
    "print(f\"Number of GA flights: {num_gas}\")\n",
    "print(f\"Number of normal flights: {num_normals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc59c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def describe_normalized(X, mask, feature_names=None):\n",
    "    X_real = X[mask == 1].reshape(-1, X.shape[-1])\n",
    "    df = pd.DataFrame(X_real, columns=feature_names if feature_names else [f\"f{i}\" for i in range(X_real.shape[1])])\n",
    "    summary = df.describe().T[[\"mean\", \"std\", \"min\", \"max\"]]\n",
    "    return summary\n",
    "summary_train = describe_normalized(X_train, m_train, feature_names=all_features)\n",
    "print(summary_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc53d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a246fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkar a divisao de gas e voos normais pelo train, val and test set\n",
    "def count_goarounds(y_split, name=\"\"):\n",
    "    binary_labels = y_split\n",
    "    n_gas = np.sum(binary_labels)\n",
    "    n_normals = len(binary_labels) - n_gas\n",
    "    print(f\"{name} set:\")\n",
    "    print(f\"Go-arounds: {n_gas}\")\n",
    "    print(f\"Normal:     {n_normals}\")\n",
    "    print(f\"Total:      {len(binary_labels)}\\n\")\n",
    "\n",
    "count_goarounds(y_train, \"Train\")\n",
    "count_goarounds(y_val, \"Validation\")\n",
    "count_goarounds(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73263b7",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd4bcda",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b946cab",
   "metadata": {},
   "source": [
    "#### tuning hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, MultiHeadAttention\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, BinaryFocalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "#reshape y because the time distributed layer expects shape: (#flights, 40, #label) instead of just (#flights, 40), time distributed layer outputs aprobablity each gate\n",
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis] #(n_train, 40, 1)\n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis] #(n_val, 40, 1)\n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis] #(n_test, 40, 1)\n",
    "\n",
    "def tuning_lstm_model(hp):\n",
    "    \n",
    "    #hyperparameters\n",
    "    lstm_units = hp.Choice('lstm_units', [128, 256]) \n",
    "    dense_units = hp.Choice('dense_units', [64, 128, 256]) \n",
    "    dropout_rate = hp.Choice('dropout_rate', [0.0, 0.1, 0.2]) \n",
    "    activation_fn = hp.Choice('activation',['ReLU', 'tanh', 'swish'])\n",
    "    learning_rate = hp.Choice('learning_rate', [0.01, 0.001])\n",
    "    attention_heads = hp.Choice('attention_heads', [2, 4, 8])\n",
    "    loss_fn = hp.Choice('loss', ['bce', 'bfl'])\n",
    "    #batch = hp.Choice('batch_size', [64, 128, 256, 512])\n",
    "    \n",
    "    #layers\n",
    "    input_shape=(X_train.shape[1], X_train.shape[2])\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=0.0)(inp)\n",
    "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "    attn = MultiHeadAttention(num_heads=attention_heads, key_dim=lstm_units)(x, x)\n",
    "    x = Dropout(dropout_rate)(attn)\n",
    "    x = TimeDistributed(Dense(dense_units, activation=activation_fn))(x)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    \n",
    "    #loss\n",
    "    if loss_fn == 'bce':\n",
    "        loss = BinaryCrossentropy() #standard cross entropy for binary labels\n",
    "    else:\n",
    "        loss = BinaryFocalCrossentropy(alpha=0.95, gamma=1) #focal loss \n",
    "        #alpha = 0.95: weights the minority class\n",
    "        #gamma = 1: focuses training on the hard to classify samples\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc94695",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#create a directory (windows)\n",
    "tuner_dir = os.path.normpath(\"C:/Users/anafi/tuner5\")\n",
    "os.makedirs(tuner_dir, exist_ok=True)\n",
    "\n",
    "#setup tuner: random search\n",
    "tuner = kt.RandomSearch(\n",
    "    tuning_lstm_model,\n",
    "    objective='val_precision', #tries to maximize the validation precision  TENTAR O F1 OR F2\n",
    "    max_trials=10, #runs 50 different sampled hyperparameter trials (50 models)             \n",
    "    executions_per_trial=2, #trains each sampled hyperparameters twice     \n",
    "    directory=tuner_dir,\n",
    "    project_name='lstm'\n",
    ")\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1072d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#early stopping \n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "#search tuner\n",
    "tuner.search(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e3789",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(\"Best hyperparameters:\")\n",
    "print(f\"LSTM units: {best_hps.get('lstm_units')}\")\n",
    "print(f\"Dense units: {best_hps.get('dense_units')}\")\n",
    "print(f\"Dropout rate: {best_hps.get('dropout_rate')}\")\n",
    "print(f\"Activation: {best_hps.get('activation')}\")\n",
    "print(f\"Learning rate: {best_hps.get('learning_rate')}\")\n",
    "print(f\"Attention heads: {best_hps.get('attention_heads')}\")\n",
    "print(f\"Loss: {best_hps.get('loss')}\")\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9bff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()\n",
    "\n",
    "#test the best model with test set\n",
    "best_model.evaluate(X_test, y_test_reshaped)\n",
    "\n",
    "#retrain best model on train + validation sets\n",
    "best_hp = tuner.get_best_hyperparameters(1)[0]\n",
    "final_model = tuning_lstm_model(best_hp)  \n",
    "final_model.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train_reshaped, y_val_reshaped]), epochs=20, batch_size=64, callbacks=[early_stop]) \n",
    "final_model.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0ac79",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054d0c10",
   "metadata": {},
   "source": [
    "#### training + testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b13154",
   "metadata": {},
   "source": [
    "##### model1: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f2504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_lstm_model(input_shape, lstm_units=128, dense_units=256, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=0.0)(inp) \n",
    "    x = LSTM(lstm_units, return_sequences=True)(x) \n",
    "    x = Dropout(dropout_rate)(x) \n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x) \n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x) \n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6099a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis] \n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]   \n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]\n",
    "\n",
    "model1 = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), lstm_units=128, dense_units=256, dropout_rate=0.2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model1.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64, \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model1.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12c0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model1.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b855782c",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887d5ace",
   "metadata": {},
   "source": [
    "##### model2: Bi-LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_lstm_model(input_shape, lstm_units=128, dense_units=256, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape) \n",
    "    x = Masking(mask_value=0.0)(inp)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x) \n",
    "    x = Dropout(dropout_rate)(x) \n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x) \n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis]  \n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]   \n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]\n",
    "\n",
    "model2 = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), lstm_units=128, dense_units=256, dropout_rate=0.2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model2.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model2.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93433a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.save('model2.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6ce576",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c545ddde",
   "metadata": {},
   "source": [
    "##### model3: GRU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc301e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, MultiHeadAttention, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_lstm_model(input_shape, gru_units=128, dense_units=256, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape) \n",
    "    x = Masking(mask_value=0.0)(inp) \n",
    "    x = GRU(gru_units, return_sequences=True)(x) \n",
    "    x = Dropout(dropout_rate)(x) \n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x) \n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x) \n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5987550",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis]  # (n_train, 20, 1)\n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]    # (n_val, 20, 1)\n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]   # (n_test, 20, 1)\n",
    "\n",
    "model3 = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), gru_units=128, dense_units=256, dropout_rate=0.1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model3.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model3.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342cf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model3.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9541ca74",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66244b30",
   "metadata": {},
   "source": [
    "##### model4: LSTM + attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1c987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, MultiHeadAttention\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_lstm_model(input_shape, lstm_units=128, dense_units=128, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=0.0)(inp)\n",
    "    x = LSTM(lstm_units, return_sequences=True)(x)\n",
    "    attn = MultiHeadAttention(num_heads=4, key_dim=lstm_units)(x, x)\n",
    "    x = Dropout(dropout_rate)(attn)\n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c13357",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis]  # (n_train, 20, 1)\n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]    # (n_val, 20, 1)\n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]   # (n_test, 20, 1)\n",
    "\n",
    "model4 = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), lstm_units=128, dense_units=128, dropout_rate=0.1)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model4.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64, \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model4.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc677392",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.save('model4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ab4b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model4 = keras.models.load_model(\"model4.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7139ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot: precision recall vs threshold and precision recall curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y_val_pred = model4.predict(X_val)\n",
    "y_val_pred = y_val_pred.squeeze(-1) \n",
    "print(\"y_val_pred shape:\", y_val_pred.shape)\n",
    "\n",
    "probs = y_val_pred[m_val == 1].flatten() \n",
    "true  = y_val_reshaped[m_val == 1].flatten() \n",
    "\n",
    "precision, recall, threshold = precision_recall_curve(true, probs)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(threshold, precision[:-1], label=\"Precision\", color=\"blue\")\n",
    "plt.plot(threshold, recall[:-1], label=\"Recall\", color=\"orange\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Precision-Recall vs Threshold (valid gates only)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(recall, precision, color=\"purple\")\n",
    "plt.xlabel(\"Recall\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(f\"Precision-Recall Curve (AP={average_precision_score(true, probs):.4f})\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the best thresholds (max f1 score) for each gate\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "import numpy as np\n",
    "\n",
    "y_val_pred_prob = model4.predict(X_val)  \n",
    "y_val_pred_prob = y_val_pred_prob.squeeze(-1)  \n",
    "\n",
    "best_thresholds = []\n",
    "\n",
    "for gate in range(y_val_pred_prob.shape[1]):\n",
    "    y_true_gate = y_val_reshaped[:, gate, 0] #true labels\n",
    "    y_prob_gate = y_val_pred_prob[:, gate] #predicated labels  \n",
    "    \n",
    "    thresholds = np.arange(0.0, 1.0, 0.001)  \n",
    "    f1_scores, f2_scores, precisions, recalls = [], [], [], []\n",
    "    \n",
    "    for t in thresholds:\n",
    "        y_pred_gate = (y_prob_gate >= t).astype(int)\n",
    "        f1 = f1_score(y_true_gate, y_pred_gate, zero_division=0)\n",
    "        f2 = fbeta_score(y_true_gate, y_pred_gate, beta=2, zero_division=0)\n",
    "        prec = precision_score(y_true_gate, y_pred_gate, zero_division=0)\n",
    "        rec = recall_score(y_true_gate, y_pred_gate, zero_division=0)\n",
    "        \n",
    "        f1_scores.append(f1)\n",
    "        f2_scores.append(f2)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "    \n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    best_thresholds.append(best_threshold)\n",
    "    \n",
    "    print(\n",
    "        f\"Gate {gate+1}: \"\n",
    "        f\"Best threshold = {best_threshold:.3f}, \"\n",
    "        f\"F1 = {f1_scores[best_idx]:.4f}, \"\n",
    "        f\"F2 = {f2_scores[best_idx]:.4f}, \"\n",
    "        f\"Precision = {precisions[best_idx]:.4f}, \"\n",
    "        f\"Recall = {recalls[best_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bbc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the confusion matrix for each gate\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "for gate in range(y_val_pred_prob.shape[1]):\n",
    "    y_true_gate = y_val_reshaped[:, gate, 0]\n",
    "    y_prob_gate = y_val_pred_prob[:, gate]\n",
    "    \n",
    "    threshold = best_thresholds[gate]\n",
    "    y_pred_gate = (y_prob_gate >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_true_gate, y_pred_gate)\n",
    "    print(f\"Gate {gate+1} confusion matrix:\")\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898000b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the importance matrix for global analysis\n",
    "\n",
    "#function: computes the permutation importance, each feature is shuffled randomly at each gate 5 times\n",
    "def global_permutation_importance(model, X_test, y_test, n_iter=5, gates=None, random_state=42):\n",
    "\n",
    "    random = np.random.default_rng(random_state)\n",
    "    n_samples, n_gates, n_features = X_test.shape\n",
    "    if gates is None:\n",
    "        gates = list(range(n_gates))\n",
    "\n",
    "    bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    original_loss = bfl(y_test, y_pred).numpy()\n",
    "\n",
    "    importance_matrix = np.zeros((n_features, len(gates)))\n",
    "\n",
    "    for gate_idx, g in enumerate(gates):\n",
    "        print(f\"Processing gate {g+1}/{n_gates}\")\n",
    "\n",
    "        for feature_idx in range(n_features):\n",
    "            scores = []\n",
    "            for _ in range(n_iter):\n",
    "                X_permutation = X_test.copy()\n",
    "                shuffled = random.permutation(X_permutation[:, g, feature_idx])\n",
    "                X_permutation[:, g, feature_idx] = shuffled\n",
    "\n",
    "                y_pred_permutation = model.predict(X_permutation, verbose=0)\n",
    "                permutation_loss = bfl(y_test, y_pred_permutation).numpy()\n",
    "\n",
    "                scores.append(original_loss - permutation_loss)\n",
    "            importance_matrix[feature_idx, gate_idx] = np.mean(scores)\n",
    "\n",
    "    return importance_matrix\n",
    "\n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]\n",
    "print(y_test_reshaped.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "subset_gates = [0, 5, 10, 15]  \n",
    "importance_matrix = global_permutation_importance(model4, X_test, y_test_reshaped, n_iter=5, gates=subset_gates)\n",
    "print(\"Importance matrix shape:\", importance_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975ff55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#function: plots the top 10 features per gate - globally\n",
    "def plot_global_analysis(importance_matrix, feature_names, gate_labels, original_loss, top_features=10, figsize=(10, 6)):\n",
    "    importance_matrix_relative = (importance_matrix / original_loss) * 100\n",
    "    importance_df = pd.DataFrame(importance_matrix_relative, index=feature_names, columns=gate_labels)\n",
    "    for gate in importance_df.columns:\n",
    "        df_sorted = (importance_df[gate].sort_values(ascending=True) .tail(top_features).reset_index())\n",
    "        df_sorted.columns = [\"Feature\", \"Importance (%)\"]\n",
    "\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.barplot(data=df_sorted, x=\"Importance (%)\", y=\"Feature\", color=\"royalblue\", zorder=2)\n",
    "        plt.xlabel(\"Relative Importance\", fontsize=16)\n",
    "        plt.ylabel(\"Feature\", fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True, zorder=1)\n",
    "        plt.show()\n",
    "\n",
    "    return importance_df \n",
    "\n",
    "feature_names = all_features\n",
    "gate_labels = [f\"Gate {g}\" for g in subset_gates]\n",
    "\n",
    "y_test_pred = model4.predict(X_test, verbose=0)\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "original_loss = bfl(y_test_reshaped, y_test_pred).numpy()\n",
    "\n",
    "importance_df_relative = plot_global_analysis(importance_matrix, feature_names, gate_labels, original_loss, top_features=10, figsize=(10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb64e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the importance matrix for local interpretability \n",
    "import numpy as np\n",
    "\n",
    "#function: receives delta which is how far a feature is from its mean, applies different transformations depending on the feature type\n",
    "def feature_function(delta, feature_name):\n",
    "    #binary indicators\n",
    "    if feature_name in [\"WS_M\", \"FG_M\", \"RA_M\", \"G_M\", \"GA_before\", \"Lead_Aircraft\"]:\n",
    "        return 1 if delta > 0 else 0\n",
    "\n",
    "    #absolute deviations\n",
    "    if feature_name in [\"Wind_Speed_M\", \"Wind_Gust_E\", \"Wind_Shear_E\", \"Deviation\", \"Energy\", \"GA_hourly\", \"GA_lasttime\", \"C_1000_E\", \"C_500_E\", \"Wind_CompU_E\", \"Wind_CompV_E\", \"Speed\",\"Correct Altitude\", \"Vertical Rate\",\"Glideslope\"]:\n",
    "        return abs(delta)\n",
    "\n",
    "    #negative deviations (higher = less risk)\n",
    "    if feature_name in [\"Speed_Diff_lt\", \"Alt_Diff_lt\"]:\n",
    "        return min(0, delta)\n",
    "    \n",
    "    #positive deviations (lower = less risk)\n",
    "    if feature_name in [\"LOS\"]:\n",
    "        return max(0, delta)\n",
    "\n",
    "    return delta\n",
    "\n",
    "#function: computes the local importance of each feature \n",
    "def local_permutation_importance(X_test, means, stds, global_matrix, feature_names, flight_idx, gates=None):\n",
    "\n",
    "    n_samples, n_gates, n_features = X_test.shape\n",
    "    if gates is None:\n",
    "        gates = list(range(n_gates))\n",
    "\n",
    "    contribution_matrix = np.zeros((n_features, len(gates)))\n",
    "    normalized_contribution = np.zeros_like(contribution_matrix) \n",
    "\n",
    "    for gate_idx, g in enumerate(gates):\n",
    "\n",
    "        for f, feature_idx in enumerate(feature_names):  \n",
    "            value = X_test[flight_idx, g, f]\n",
    "            mean, std = means[f], stds[f]\n",
    "            delta = (value - mean) / (std if std > 0 else 1) \n",
    "\n",
    "            feature_val = feature_function(delta, feature_idx)\n",
    "            beta = gates.index(gate_idx)\n",
    "            contribution_matrix[f, gate_idx] = feature_val * max(0, global_matrix[f, beta]) \n",
    "\n",
    "        denominator = contribution_matrix[:, gate_idx].sum()\n",
    "        if denominator > 0:\n",
    "           normalized_contribution[:, gate_idx] = contribution_matrix[:, gate_idx] / denominator * 100\n",
    "\n",
    "    return contribution_matrix, normalized_contribution\n",
    "\n",
    "flight_idx = 2184 #true positive, change for the false positive, true negative and false negative\n",
    "#tp_idx = 2184 - yellowgreen\n",
    "#tn_idx = 4 - royalblue\n",
    "#fp_idx = 7687 - orange\n",
    "#fn_idx = 1016 - tomato\n",
    "\n",
    "importance_df = pd.DataFrame(importance_matrix, index=feature_names, columns=gate_labels)\n",
    "global_matrix = importance_df.to_numpy() \n",
    "means = X_test.mean(axis=(0, 1))   \n",
    "stds = X_test.std(axis=(0, 1))\n",
    "\n",
    "contribution_matrix, normalized_contribution = local_permutation_importance(X_test, means, stds, global_matrix, all_features, flight_idx, gates=subset_gates)\n",
    "\n",
    "gate = 0\n",
    "gate_pos = subset_gates.index(gate) \n",
    "\n",
    "df_local = pd.DataFrame({\"Feature\": all_features, \"Contribution matrix\": contribution_matrix[:, gate_pos], \"Normalized contribution (%)\": normalized_contribution[:, gate_pos]}).sort_values(\"Normalized contribution (%)\", ascending=False)\n",
    "print(df_local.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21701fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "#function: plots the top 5 features per gate - locally\n",
    "def plot_local_interpretability(alpha, feature_names, gates, top_features=5):\n",
    "\n",
    "    for g_idx, g_real in enumerate(gates):\n",
    "        df = pd.DataFrame({\n",
    "            \"Feature\": feature_names,\n",
    "            \"Alpha (%)\": alpha[:, g_idx]  \n",
    "        }).sort_values(\"Alpha (%)\", ascending=False).head(top_features)\n",
    "\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        sns.barplot(data=df, x=\"Alpha (%)\", y=\"Feature\", color=\"tomato\", zorder=2)\n",
    "        plt.xlabel(\"Contribution (%)\", fontsize=16)\n",
    "        plt.ylabel(\"Feature\", fontsize=16)\n",
    "        plt.xticks(fontsize=16)\n",
    "        plt.yticks(fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.grid(True, zorder=1)\n",
    "        plt.show()\n",
    "\n",
    "contribution_matrix, normalized_contribution = local_permutation_importance(X_test, means, stds, global_matrix, feature_names, flight_idx=1016, gates=subset_gates)\n",
    "\n",
    "plot_local_interpretability(normalized_contribution, feature_names, subset_gates, top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3587973",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176b9c5",
   "metadata": {},
   "source": [
    "##### model5: Bi-LSTM + attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f502d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, MultiHeadAttention, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_bilstm_model(input_shape, lstm_units=128, dense_units=256, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape)\n",
    "    x = Masking(mask_value=0.0)(inp)\n",
    "    x = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "    attn = MultiHeadAttention(num_heads=4, key_dim=(lstm_units))(x, x)\n",
    "    x = Dropout(dropout_rate)(attn)\n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x) \n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d103466",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis]  \n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]   \n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis] \n",
    "\n",
    "model5 = build_bilstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), lstm_units=128, dense_units=256, dropout_rate=0.2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model5.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model5.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235a3f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model5.save('model5.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a01857f",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40146eed",
   "metadata": {},
   "source": [
    "##### model6: GRU + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e9c3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Masking, LSTM, Dropout, TimeDistributed, Dense, MultiHeadAttention, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.losses import BinaryFocalCrossentropy\n",
    "\n",
    "bfl = BinaryFocalCrossentropy(alpha=0.95, gamma=1)\n",
    "\n",
    "def build_lstm_model(input_shape, lstm_units=128, dense_units=256, dropout_rate=0.1):\n",
    "    inp = Input(shape=input_shape) \n",
    "    x = Masking(mask_value=0.0)(inp) \n",
    "    x = GRU(lstm_units, return_sequences=True)(x) \n",
    "    attn = MultiHeadAttention(num_heads=4, key_dim=lstm_units)(x, x)\n",
    "    x = Dropout(dropout_rate)(attn) \n",
    "    x = TimeDistributed(Dense(dense_units, activation='relu'))(x)\n",
    "    out = TimeDistributed(Dense(1, activation='sigmoid'))(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "    model.compile( \n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss=bfl,\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='auc')])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis]  \n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis]  \n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis]   \n",
    "\n",
    "model6 = build_lstm_model(input_shape=(X_train.shape[1], X_train.shape[2]), lstm_units=128, dense_units=256, dropout_rate=0.2)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "history = model6.fit(\n",
    "    X_train, y_train_reshaped,\n",
    "    validation_data=(X_val, y_val_reshaped),\n",
    "    epochs=50, \n",
    "    batch_size=64, \n",
    "    callbacks=[early_stop],\n",
    "    verbose=1)\n",
    "\n",
    "model6.evaluate(X_test, y_test_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36442ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model6.save('model6.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eab574",
   "metadata": {},
   "source": [
    "#### ----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19892859",
   "metadata": {},
   "source": [
    "#### Data analysis + plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d8589",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_reshaped = np.tile(y_train[:, None], (1, X_train.shape[1]))[..., np.newaxis] \n",
    "y_val_reshaped   = np.tile(y_val[:, None],   (1, X_val.shape[1]))[..., np.newaxis] \n",
    "y_test_reshaped  = np.tile(y_test[:, None],  (1, X_test.shape[1]))[..., np.newaxis] \n",
    "\n",
    "print(y_test.shape)\n",
    "print(y_test_reshaped.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4645eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the best thresholds for each of the models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, fbeta_score\n",
    "import tensorflow as tf\n",
    "\n",
    "models = {\n",
    "    \"LSTM\": tf.keras.models.load_model(\"model1.keras\"),\n",
    "    \"Bi-LSTM\": tf.keras.models.load_model(\"model2.keras\"),\n",
    "    \"GRU\": tf.keras.models.load_model(\"model3.keras\"),\n",
    "    \"LSTM + attention\": tf.keras.models.load_model(\"model4.keras\"),\n",
    "    \"Bi-LSTM + attention\": tf.keras.models.load_model(\"model5.keras\"),\n",
    "    \"GRU + attention\": tf.keras.models.load_model(\"model6.keras\"),\n",
    "}\n",
    "\n",
    "n_gates = 40\n",
    "threshold_candidates = np.arange(0.0, 1.0, 0.001)\n",
    "best_thresholds = {name: [] for name in models.keys()}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_val_pred_prob = model.predict(X_val).squeeze(-1) \n",
    "    \n",
    "    for gate in range(n_gates):\n",
    "        y_true_gate = y_val_reshaped[:, gate, 0]\n",
    "        y_prob_gate = y_val_pred_prob[:, gate]\n",
    "\n",
    "        f1_scores = []\n",
    "        for t in threshold_candidates:\n",
    "            y_pred_gate = (y_prob_gate >= t).astype(int)\n",
    "            f1 = f1_score(y_true_gate, y_pred_gate, zero_division=0)\n",
    "            f1_scores.append(f1)\n",
    "\n",
    "        best_idx = np.argmax(f1_scores)\n",
    "        best_thresholds[name].append(threshold_candidates[best_idx])\n",
    "\n",
    "\n",
    "f1_model = {name: [] for name in models.keys()}\n",
    "for name, model in models.items():\n",
    "    y_test_pred_prob = model.predict(X_test).squeeze(-1)\n",
    "\n",
    "    for gate in range(n_gates):\n",
    "        y_true_gate = y_test_reshaped[:, gate, 0]\n",
    "        threshold = best_thresholds[name][gate]\n",
    "        y_pred_gate = (y_test_pred_prob[:, gate] >= threshold).astype(int)\n",
    "\n",
    "        f1 = f1_score(y_true_gate, y_pred_gate, zero_division=0)\n",
    "        f1_model[name].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7cd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot: model's f1 score per gate\n",
    "plt.figure(figsize=(12, 6))\n",
    "gates = np.arange(0, n_gates)\n",
    "\n",
    "for name, scores in f1_model.items():\n",
    "    plt.plot(gates, scores, marker='o', label=name)\n",
    "\n",
    "plt.xlabel(\"Gate (0 = 10NM, 39 = 0.25NM)\", fontsize=16)\n",
    "plt.ylabel(\"F1 Score\", fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e73fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#table with examples of true positives, false positives, true negatives and false negatives\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"LSTM + attention\"\n",
    "model = models[model_name]\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test).squeeze(-1)\n",
    "y_pred_bin = np.zeros_like(y_test_pred_prob, dtype=int)\n",
    "\n",
    "for g in range(n_gates):\n",
    "    threshold = best_thresholds[model_name][g]\n",
    "    y_pred_bin[:, g] = (y_test_pred_prob[:, g] >= threshold).astype(int)\n",
    "\n",
    "tp_indices = np.where((y_test_reshaped[:, :, 0].max(axis=1) == 1) & (y_pred_bin.max(axis=1) == 1))[0]\n",
    "fn_indices = np.where((y_test_reshaped[:, :, 0].max(axis=1) == 1) &(y_pred_bin.max(axis=1) == 0))[0]\n",
    "fp_indices = np.where((y_test_reshaped[:, :, 0].max(axis=1) == 0) &  (y_pred_bin.max(axis=1) == 1))[0]\n",
    "tn_indices = np.where((y_test_reshaped[:, :, 0].max(axis=1) == 0) &(y_pred_bin.max(axis=1) == 0))[0]\n",
    "\n",
    "n_display = 10\n",
    "tp_sample = tp_indices[:n_display]\n",
    "fn_sample = fn_indices[:n_display]\n",
    "fp_sample = fp_indices[:n_display]\n",
    "tn_sample = tn_indices[:n_display]\n",
    "\n",
    "def create_pred_table(indices, y_pred_prob, thresholds):\n",
    "    table = []\n",
    "    for idx in indices:\n",
    "        row = [idx]  # first column = flight index\n",
    "        for g in range(n_gates):\n",
    "            prob = y_pred_prob[idx, g]\n",
    "            pred = int(prob >= thresholds[g])\n",
    "            row.append(f\"{prob:.2f} ({pred})\")\n",
    "        table.append(row)\n",
    "    df = pd.DataFrame(table, columns=[\"Flight_Index\"] + [f\"Gate {g+1}\" for g in range(n_gates)])\n",
    "    return df\n",
    "\n",
    "tp_table = create_pred_table(tp_sample, y_test_pred_prob, best_thresholds[model_name])\n",
    "fn_table = create_pred_table(fn_sample, y_test_pred_prob, best_thresholds[model_name])\n",
    "fp_table = create_pred_table(fp_sample, y_test_pred_prob, best_thresholds[model_name])\n",
    "tn_table = create_pred_table(tn_sample, y_test_pred_prob, best_thresholds[model_name])\n",
    "\n",
    "print(\"True Positives:\")\n",
    "display(tp_table)\n",
    "\n",
    "print(\"False Negatives:\")\n",
    "display(fn_table)\n",
    "\n",
    "print(\"False Positives:\")\n",
    "display(fp_table)\n",
    "\n",
    "print(\"True Negatives:\")\n",
    "display(tn_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find info about a true positive, false positive, true negative and false negative\n",
    "tp_id = ids_test[2184]\n",
    "print(\"True Positive flight ID:\", tp_id)\n",
    "df_flight = df[df[\"Flight_Index\"] == tp_id]\n",
    "date_indices = df_flight[\"Date\"].iloc[0]\n",
    "landing_indices = df_flight[\"Landing_Index\"].unique()\n",
    "icao_indices = df_flight[\"ICAO\"].unique()\n",
    "ga_indices = df_flight[\"Go_Around_Label\"].max()\n",
    "runway_indices = df_flight[\"Runway\"].unique()\n",
    "print(f\"Flight {tp_id} has these landings: {landing_indices} and {icao_indices}, {ga_indices}, {date_indices}, {runway_indices}\")\n",
    "\n",
    "tn_id = ids_test[4]\n",
    "print(\"True Negative flight ID:\", tn_id)\n",
    "df_flight = df[df[\"Flight_Index\"] == tn_id]\n",
    "date_indices = df_flight[\"Date\"].iloc[0]\n",
    "landing_indices = df_flight[\"Landing_Index\"].unique()\n",
    "icao_indices = df_flight[\"ICAO\"].unique()\n",
    "ga_indices = df_flight[\"Go_Around_Label\"].max()\n",
    "runway_indices = df_flight[\"Runway\"].unique()\n",
    "print(f\"Flight {tn_id} has these landings: {landing_indices} and {icao_indices}, {ga_indices}, {date_indices}, {runway_indices}\")\n",
    "\n",
    "fp_id = ids_test[7687]\n",
    "print(\"False Positive flight ID:\", fp_id)\n",
    "df_flight = full_df[full_df[\"Flight_Index\"] == fp_id]\n",
    "date_indices = df_flight[\"Date\"].iloc[0]\n",
    "landing_indices = df_flight[\"Landing_Index\"].unique()\n",
    "icao_indices = df_flight[\"ICAO\"].unique()\n",
    "ga_indices = df_flight[\"Go_Around_Label\"].max()\n",
    "runway_indices = df_flight[\"Runway\"].unique()\n",
    "print(f\"Flight {fp_id} has these landings: {landing_indices} and {icao_indices}, {ga_indices}, {date_indices}, {runway_indices}\")\n",
    "\n",
    "fn_id = ids_test[1016]\n",
    "print(\"False Negative flight ID:\", fn_id)\n",
    "df_flight = df[df[\"Flight_Index\"] == fn_id]\n",
    "date_indices = df_flight[\"Date\"].iloc[0]\n",
    "landing_indices = df_flight[\"Landing_Index\"].unique()\n",
    "icao_indices = df_flight[\"ICAO\"].unique()\n",
    "ga_indices = df_flight[\"Go_Around_Label\"].max()\n",
    "runway_indices = df_flight[\"Runway\"].unique()\n",
    "print(f\"Flight {fn_id} has these landings: {landing_indices} and {icao_indices}, {ga_indices}, {date_indices}, {runway_indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots: flight and weather features for a a true positive, false positive, true negative and false negative\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model_name = \"LSTM + attention\"\n",
    "model = models[model_name]\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test).squeeze(-1) \n",
    "y_pred_bin = np.zeros_like(y_test_pred_prob, dtype=int)\n",
    "\n",
    "for g in range(n_gates):\n",
    "    threshold = best_thresholds[model_name][g]\n",
    "    y_pred_bin[:, g] = (y_test_pred_prob[:, g] >= threshold).astype(int)\n",
    "\n",
    "#examples\n",
    "tp_idx = 2184\n",
    "tn_idx = 4 \n",
    "fp_idx = 7687 \n",
    "fn_idx = 1016 \n",
    "\n",
    "tp_seq = X_test_real[tp_idx]\n",
    "tn_seq = X_test_real[tn_idx]\n",
    "fn_seq = X_test_real[fn_idx]\n",
    "fp_seq = X_test_real[fp_idx]\n",
    "\n",
    "tp_mask = m_test[tp_idx]\n",
    "tn_mask = m_test[tn_idx]\n",
    "fn_mask = m_test[fn_idx]\n",
    "fp_mask = m_test[fp_idx]\n",
    "\n",
    "tp_seq_real = tp_seq[tp_mask==1]\n",
    "tn_seq_real = tn_seq[tn_mask==1]\n",
    "fn_seq_real = fn_seq[fn_mask==1]\n",
    "fp_seq_real = fp_seq[fp_mask==1]\n",
    "\n",
    "static_features = [\"Wind_Speed_M\", \"Wind_Gust_E\", \"Wind_Shear_E\", \"C_1000_E\", \"C_500_E\", \"WS_M\", \"FG_M\", \"RA_M\", \"G_M\", \"Wind_CompU_E\", \"Wind_CompV_E\", \"LOS\", \"GA_before\", \"GA_hourly\", \"GA_lasttime\", \"Speed_Diff_lt\", \"Alt_Diff_lt\", \"Lead_Aircraft\"]\n",
    "\n",
    "dynamic_features = [\"Correct Altitude\", \"Speed\", \"Vertical Rate\", \"Glideslope\", \"Deviation\", \"Energy\"]\n",
    "\n",
    "mpl.rcParams[\"axes.spines.top\"] = False\n",
    "mpl.rcParams[\"axes.spines.right\"] = False\n",
    "mpl.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "colors = {\n",
    "    \"True Positive (GA=1)\":\"#2ca02c\",  \n",
    "    \"True Negative (Normal=0)\":\"#1f77b4\", \n",
    "    \"False Negative (GA=0)\": \"#d62728\", \n",
    "    \"False Positive (Normal=1)\": \"#ff7f0e\", \n",
    "}\n",
    "\n",
    "for feature in dynamic_features:\n",
    "    feature_idx = all_features.index(feature)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.plot(tp_seq_real[:, feature_idx], label=\"True Positive (GA=1)\", color=colors[\"True Positive (GA=1)\"], linewidth=2, marker=\"o\", markersize=4)\n",
    "    plt.plot(tn_seq_real[:, feature_idx], label=\"True Negative (Normal=0)\", color=colors[\"True Negative (Normal=0)\"], linewidth=2, marker=\"s\", markersize=4)\n",
    "    plt.plot(fn_seq_real[:, feature_idx], label=\"False Negative (GA=0)\", color=colors[\"False Negative (GA=0)\"], linewidth=2, marker=\"^\", markersize=4)\n",
    "    plt.plot(fp_seq_real[:, feature_idx], label=\"False Positive (Normal=1)\", color=colors[\"False Positive (Normal=1)\"], linewidth=2, marker=\"D\", markersize=4)\n",
    "\n",
    "    plt.xlabel(\"Gate\", fontsize=16)\n",
    "    plt.ylabel(feature, fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.grid(alpha=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for feature in static_features:\n",
    "    feature_idx = all_features.index(feature)\n",
    "\n",
    "    values = {\n",
    "        \"True Positive (GA=1)\": tp_seq_real[0, feature_idx],\n",
    "        \"True Negative (Normal=0)\": tn_seq_real[0, feature_idx],\n",
    "        \"False Negative (GA=0)\": fn_seq_real[0, feature_idx],\n",
    "        \"False Positive (Normal=1)\": fp_seq_real[0, feature_idx],\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    bars = plt.barh(list(values.keys()), list(values.values()), color=[colors[k] for k in values.keys()], alpha=0.85, zorder=2)\n",
    "    \n",
    "    for bar in bars:\n",
    "        width = bar.get_width()\n",
    "        plt.text(width * 0.95, bar.get_y() + bar.get_height()/2, f\"{width:.2f}\", ha=\"right\", va=\"center\", color=\"white\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    plt.xlabel(feature, fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.grid(alpha=0.5, zorder=1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bcd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot: wind directions for a a true positive, false positive, true negative and false negative\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_static_wind(tp_seq_real, tn_seq_real, fn_seq_real, fp_seq_real, all_features, colors):\n",
    "    windu_idx = all_features.index(\"Wind_CompU_E\")\n",
    "    windv_idx = all_features.index(\"Wind_CompV_E\")\n",
    "\n",
    "    winds = {\n",
    "        \"True Positive (GA=1)\": (tp_seq_real[0, windu_idx], tp_seq_real[0, windv_idx]),\n",
    "        \"True Negative (Normal=0)\": (tn_seq_real[0, windu_idx], tn_seq_real[0, windv_idx]),\n",
    "        \"False Negative (GA=0)\": (fn_seq_real[0, windu_idx], fn_seq_real[0, windv_idx]),\n",
    "        \"False Positive (Normal=1)\": (fp_seq_real[0, windu_idx], fp_seq_real[0, windv_idx]),\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    for label, (u, v) in winds.items():\n",
    "        magnitude = np.sqrt(u**2 + v**2)\n",
    "        angle = np.arctan2(v, u) \n",
    "        ax.bar(angle, magnitude, width=0.2, color=colors[label], alpha=0.6, label=label)\n",
    "\n",
    "    compass_labels = [\"E\", \"NE\", \"N\", \"NW\", \"W\", \"SW\", \"S\", \"SE\"]\n",
    "    ax.set_xticks(np.deg2rad(np.arange(0, 360, 45)))\n",
    "    ax.set_xticklabels(compass_labels)\n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "    ax.set_theta_direction(1)\n",
    "\n",
    "    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.3, 1.1), fontsize=10)\n",
    "    plt.show()\n",
    "\n",
    "if \"Wind_CompU_E\" in static_features and \"Wind_CompV_E\" in static_features:\n",
    "    plot_static_wind(tp_seq_real, tn_seq_real, fn_seq_real, fp_seq_real, all_features, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c451ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot: go arounds' confidence values per gate \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "model_name = \"LSTM + attention\"\n",
    "model = models[model_name]\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test).squeeze(-1) \n",
    "gates_plot = [0, 9, 19, 29]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(gates_plot), figsize=(5 * len(gates_plot), 4))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, g in enumerate(gates_plot):\n",
    "    ax = axes[i]\n",
    "    probs = y_test_pred_prob[:, g]\n",
    "    \n",
    "    counts_ga, bins, _ = ax.hist(probs[y_test==1], bins=50, alpha=0.5, color=\"red\")\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    x = np.linspace(0, 1, 200)\n",
    "    kde_ga = gaussian_kde(probs[y_test==1])\n",
    "    ax.plot(x, kde_ga(x) * len(probs[y_test==1]) * bin_width, color=\"red\", lw=2)\n",
    "\n",
    "    ax.set_title(f\"Gate {g+1}\", fontsize=16)\n",
    "    ax.set_xlabel(\"Confidence values\", fontsize=16)\n",
    "    ax.set_ylabel(\"Count\", fontsize=16)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1449289",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot: normal landings' confidence values per gate\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "model_name = \"LSTM + attention\"\n",
    "model = models[model_name]\n",
    "\n",
    "y_test_pred_prob = model.predict(X_test).squeeze(-1) \n",
    "gates_plot = [0, 9, 19, 29]\n",
    "\n",
    "fig, axes = plt.subplots(1, len(gates_plot), figsize=(5 * len(gates_plot), 4))\n",
    "axes = np.array(axes).flatten()\n",
    "\n",
    "for i, g in enumerate(gates_plot):\n",
    "    ax = axes[i]\n",
    "    probs = y_test_pred_prob[:, g]\n",
    "\n",
    "    counts_norm, _, _ = ax.hist(probs[y_test==0], bins=bins, alpha=0.5, color=\"blue\")\n",
    "    bin_width = bins[1] - bins[0]\n",
    "\n",
    "    x = np.linspace(0, 1, 200)\n",
    "    kde_norm = gaussian_kde(probs[y_test==0])\n",
    "    ax.plot(x, kde_norm(x) * len(probs[y_test==0]) * bin_width, color=\"blue\", lw=2)\n",
    "\n",
    "    ax.set_title(f\"Gate {g+1}\", fontsize=16)\n",
    "    ax.set_xlabel(\"Confidence values\", fontsize=16)\n",
    "    ax.set_ylabel(\"Count\", fontsize=16)\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, None)\n",
    "    ax.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
